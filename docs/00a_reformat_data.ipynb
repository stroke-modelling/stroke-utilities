{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reformat data\n",
    "\n",
    "This notebook reformats the cleaned up SSNAP data for use with machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Set the maximum number of columns to 100\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up paths and filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Paths:\n",
    "    '''Singleton object for storing paths to data and database.'''\n",
    "\n",
    "    data_read_path: str = '~/ssnap_data/'\n",
    "    data_read_filename: str = './clean_samuel_ssnap_extract_v2.csv'\n",
    "    data_save_path: str = './stroke_utilities/data/'\n",
    "    data_save_filename: str = 'reformatted_data_thrombolysis_decision.csv'\n",
    "    notebook: str = ''\n",
    "\n",
    "paths = Paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hosptial_thrombolysis_threshold = 10\n",
    "min_hospital_admission_threshold = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358993, 70)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = os.path.join(paths.data_read_path, paths.data_read_filename)\n",
    "data = pd.read_csv(filename)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove patients with no recorded prior disability\n",
    "mask = data['prior_disability'] >= 0\n",
    "data = data[mask]\n",
    "\n",
    "# Remove records with negative onset_to_arrival_time\n",
    "mask = data['onset_to_arrival_time'] <= 0\n",
    "mask =  mask == False\n",
    "data = data[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include hospitals with more than 250 admissions, and give atleast 10 thrombolysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_stroke_team(data, min_threshold, result):\n",
    "    \"\"\"\n",
    "    Returns the dataframe with only the stroke teams that pass a minimum threshold.\n",
    "    Currently use to have at least 250 admissions, and give thrombolysis at least 10 times\n",
    "    data [dataframe]: contains all the hospital data\n",
    "    min_threshold [float]: threshold above which stroke team needs to be to stay in data\n",
    "    result [series]: contains value per stroke team, to be compared against the minimum threshold\n",
    "    \"\"\"\n",
    "\n",
    "    mask = result >= min_threshold\n",
    "    stroke_team_keep = list(result[mask].index)\n",
    "    data = data[data['stroke_team'].isin(stroke_team_keep)]\n",
    "\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358925, 70)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = data.groupby(['stroke_team'])['thrombolysis'].sum()\n",
    "data = filter_stroke_team(data, min_hosptial_thrombolysis_threshold, result)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358925, 70)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = data.groupby(['stroke_team'])['stroke_team'].count()\n",
    "data = filter_stroke_team(data, min_hospital_admission_threshold, result)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's an assumption that patients with NK recorded for feature S1OnsetTimeType have their stroke onset at midnight, and so their OnsettoArrivalMinutes taken as the duration from midnight. \n",
    "\n",
    "Currently these patients have a 0 for 'onset_known' (a value of 1 is for precise and best estimate).\n",
    "\n",
    "Keep patients with 'onset_known' = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = data['onset_known'] == 1\n",
    "data = data[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing features\n",
    "Set up a list of features to remove (as there are others) and remove at same time.\n",
    "\n",
    "Remove 'onset\\_known', as all patients have same value (only kept those with a value of 1 ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features = ['onset_known']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove anticolagulant types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features.append('afib_vit_k_anticoagulant')\n",
    "remove_features.append('afib_doac_anticoagulant')\n",
    "remove_features.append('afib_heparin_anticoagulant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features.append('discharge_destination')\n",
    "remove_features.append('death')\n",
    "remove_features.append('disability_6_month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove features about ambulance times (not fully filled in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_features.append('call_to_ambulance_arrival_time')\n",
    "remove_features.append('ambulance_on_scene_time')\n",
    "remove_features.append('ambulance_travel_to_hospital_time')\n",
    "remove_features.append('ambulance_wait_time_at_hospital')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(remove_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add anonymised stroke team code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of teams\n",
    "teams = list(set(data['stroke_team']))\n",
    "# Shuffle into random order\n",
    "random.seed(42)\n",
    "random.shuffle(teams)\n",
    "# Create dictionary\n",
    "teams_code_dict = dict()\n",
    "for i, j in enumerate(teams):\n",
    "    teams_code_dict[j] = i + 1\n",
    "# Save teams ID\n",
    "col_names = ['stroke_team', 'team_code']\n",
    "teams_code_df = pd.DataFrame(\n",
    "    teams_code_dict.items(), columns=col_names)\n",
    "filename = os.path.join(paths.data_save_path, 'team_code.csv')\n",
    "teams_code_df.to_csv(filename,index=False)\n",
    "# Apply coding to data\n",
    "data['stroke_team_id'] = data['stroke_team'].map(teams_code_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned data ready for machine learning (predict the discharge outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = paths.data_save_path + paths.data_save_filename\n",
    "data.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add stroke team code to cleaned data file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = paths.data_save_path + 'clean_samuel_ssnap_extract_v2.csv'\n",
    "clean_data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data['stroke_team_id'] = clean_data['stroke_team'].map(teams_code_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "f85b883bff9a8a9f39576b94acbdf6672b3dc17c35647e7395f81e785740a4b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
